{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5626d4f8-86ee-4459-bb88-61d4eb3f2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 0.00GB allocated / 0.00GB reserved / 85.03GB total\n",
      "Memory setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 1: MEMORY SETUP - RUN THIS FIRST =====\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "# Set memory allocation strategy\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Monitor GPU memory\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated / {reserved:.2f}GB reserved / {total:.2f}GB total\")\n",
    "    \n",
    "print_gpu_memory()\n",
    "print(\"Memory setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06904c39-b325-4c29-8b2b-d87276b0da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset found at ./dataset\n",
      "Device: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "GPU Memory: 85.03 GB\n",
      "Loading datasets...\n",
      "Class mapping: {'normal': 0, 'pancreatic_tumor': 1}\n",
      "Total training samples: 999\n",
      "Total test samples: 412\n",
      "Search train: 799, Search val: 200\n",
      "\n",
      "Creating search network...\n",
      "Model parameters: 76,460\n",
      "\n",
      "Starting architecture search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s, loss=0.569, arch_loss=0.562, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.5464\n",
      "  Arch Loss: 0.5593\n",
      "  Entropy: 1.7918\n",
      "  Current Genotype: ['dil_conv_3x3', 'sep_conv_5x5', 'sep_conv_3x3', 'dil_conv_3x3', 'sep_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'sep_conv_5x5']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.620, arch_loss=0.431, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.5308\n",
      "  Arch Loss: 0.5291\n",
      "  Entropy: 1.7918\n",
      "  Current Genotype: ['sep_conv_3x3', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'sep_conv_5x5']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 50/50 [00:47<00:00,  1.06it/s, loss=0.677, arch_loss=0.478, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 0.5447\n",
      "  Arch Loss: 0.5602\n",
      "  Entropy: 1.7917\n",
      "  Current Genotype: ['sep_conv_3x3', 'sep_conv_5x5', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'sep_conv_5x5']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.359, arch_loss=0.440, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 0.5291\n",
      "  Arch Loss: 0.5443\n",
      "  Entropy: 1.7917\n",
      "  Current Genotype: ['sep_conv_5x5', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.385, arch_loss=0.423, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 0.5006\n",
      "  Arch Loss: 0.5150\n",
      "  Entropy: 1.7917\n",
      "  Current Genotype: ['sep_conv_5x5', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'sep_conv_3x3', 'sep_conv_3x3', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.339, arch_loss=0.443, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "  Train Loss: 0.5285\n",
      "  Arch Loss: 0.5065\n",
      "  Entropy: 1.7917\n",
      "  Current Genotype: ['sep_conv_5x5', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'sep_conv_3x3', 'sep_conv_3x3', 'sep_conv_5x5', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 50/50 [00:46<00:00,  1.08it/s, loss=0.358, arch_loss=0.413, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "  Train Loss: 0.4670\n",
      "  Arch Loss: 0.4776\n",
      "  Entropy: 1.7917\n",
      "  Current Genotype: ['sep_conv_5x5', 'sep_conv_5x5', 'sep_conv_5x5', 'dil_conv_3x3', 'sep_conv_3x3', 'sep_conv_3x3', 'sep_conv_3x3', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.509, arch_loss=0.469, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "  Train Loss: 0.4556\n",
      "  Arch Loss: 0.4339\n",
      "  Entropy: 1.7916\n",
      "  Current Genotype: ['sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_3x3', 'dil_conv_3x3', 'avg_pool_3x3', 'sep_conv_3x3', 'avg_pool_3x3', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.400, arch_loss=0.543, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "  Train Loss: 0.4393\n",
      "  Arch Loss: 0.4308\n",
      "  Entropy: 1.7916\n",
      "  Current Genotype: ['sep_conv_3x3', 'sep_conv_5x5', 'avg_pool_3x3', 'dil_conv_3x3', 'avg_pool_3x3', 'sep_conv_3x3', 'avg_pool_3x3', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s, loss=0.584, arch_loss=0.487, entropy=1.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  Train Loss: 0.4390\n",
      "  Arch Loss: 0.4225\n",
      "  Entropy: 1.7916\n",
      "  Current Genotype: ['sep_conv_5x5', 'dil_conv_3x3', 'avg_pool_3x3', 'dil_conv_3x3', 'avg_pool_3x3', 'sep_conv_3x3', 'avg_pool_3x3', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "\n",
      "\n",
      "==================================================\n",
      "FINAL GENOTYPE:\n",
      "['sep_conv_5x5', 'dil_conv_3x3', 'avg_pool_3x3', 'dil_conv_3x3', 'avg_pool_3x3', 'sep_conv_3x3', 'avg_pool_3x3', 'avg_pool_3x3', 'dil_conv_3x3']\n",
      "==================================================\n",
      "\n",
      "Saved: final_alphas.npy, genotype.txt, search_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Fixed DARTS Implementation for Pancreatic Tumor Classification\n",
    "# Addresses: operation collapse, proper bi-level optimization, entropy regularization\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "SEED = 42\n",
    "\n",
    "# Dataset paths - adjust based on your setup\n",
    "DATA_ROOT = \"./dataset\"  # Change to your dataset location\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_ROOT, \"test\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    print(f\"⚠️  WARNING: Dataset not found at {DATA_ROOT}\")\n",
    "    print(\"Please upload your dataset or update DATA_ROOT path\")\n",
    "else:\n",
    "    print(f\"✓ Dataset found at {DATA_ROOT}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# GPU Memory optimization\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    # Enable memory efficient settings\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ==================== PRIMITIVES ====================\n",
    "PRIMITIVES = [\n",
    "    \"sep_conv_3x3\",\n",
    "    \"sep_conv_5x5\",\n",
    "    \"dil_conv_3x3\",\n",
    "    \"avg_pool_3x3\",\n",
    "    \"max_pool_3x3\",\n",
    "    \"skip_connect\",\n",
    "]\n",
    "\n",
    "# ==================== OPERATIONS ====================\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, C, kernel_size):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.op = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(C, C, kernel_size=kernel_size, padding=padding, groups=C, bias=False),\n",
    "            nn.Conv2d(C, C, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class DilConv(nn.Module):\n",
    "    def __init__(self, C, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        padding = dilation * (kernel_size - 1) // 2\n",
    "        self.op = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(C, C, kernel_size=kernel_size, padding=padding, \n",
    "                     dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class PoolBN(nn.Module):\n",
    "    def __init__(self, C, pool_type='max'):\n",
    "        super().__init__()\n",
    "        if pool_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        else:\n",
    "            self.pool = nn.AvgPool2d(3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(C)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(x))\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_op(name, C):\n",
    "    \"\"\"Get operation by name\"\"\"\n",
    "    if name == \"sep_conv_3x3\":\n",
    "        return SepConv(C, 3)\n",
    "    elif name == \"sep_conv_5x5\":\n",
    "        return SepConv(C, 5)\n",
    "    elif name == \"dil_conv_3x3\":\n",
    "        return DilConv(C, 3, 2)\n",
    "    elif name == \"avg_pool_3x3\":\n",
    "        return PoolBN(C, 'avg')\n",
    "    elif name == \"max_pool_3x3\":\n",
    "        return PoolBN(C, 'max')\n",
    "    elif name == \"skip_connect\":\n",
    "        return Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown operation: {name}\")\n",
    "\n",
    "# ==================== MIXED OPERATION ====================\n",
    "class MixedOp(nn.Module):\n",
    "    \"\"\"Weighted combination of operations\"\"\"\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.ops = nn.ModuleList([get_op(name, C) for name in PRIMITIVES])\n",
    "    \n",
    "    def forward(self, x, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor\n",
    "            weights: softmax weights for operations\n",
    "        \"\"\"\n",
    "        return sum(w * op(x) for w, op in zip(weights, self.ops))\n",
    "\n",
    "# ==================== CELL ====================\n",
    "class Cell(nn.Module):\n",
    "    \"\"\"DARTS cell with mixed operations\"\"\"\n",
    "    def __init__(self, C, num_nodes=4):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Create mixed ops for each intermediate node\n",
    "        # Each node takes input from all previous nodes\n",
    "        self.ops = nn.ModuleList()\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 2):  # i+2 because we have 2 input nodes\n",
    "                self.ops.append(MixedOp(C))\n",
    "    \n",
    "    def forward(self, x, alphas):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor\n",
    "            alphas: architecture parameters\n",
    "        \"\"\"\n",
    "        states = [x, x]  # Two initial states\n",
    "        offset = 0\n",
    "        \n",
    "        for i in range(self.num_nodes):\n",
    "            s = 0\n",
    "            for j, h in enumerate(states):\n",
    "                weights = F.softmax(alphas[offset + j], dim=-1)\n",
    "                s = s + self.ops[offset + j](h, weights)\n",
    "            offset += len(states)\n",
    "            states.append(s)\n",
    "        \n",
    "        # Concatenate all intermediate nodes\n",
    "        return sum(states[-self.num_nodes:]) / self.num_nodes\n",
    "\n",
    "# ==================== SEARCH NETWORK ====================\n",
    "class SearchNetwork(nn.Module):\n",
    "    def __init__(self, C=16, num_cells=8, num_nodes=4, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.num_cells = num_cells\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Cells\n",
    "        self.cells = nn.ModuleList([Cell(C, num_nodes) for _ in range(num_cells)])\n",
    "        \n",
    "        # Classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(C, num_classes)\n",
    "        \n",
    "        # Architecture parameters\n",
    "        self.num_edges = sum(range(2, num_nodes + 2))\n",
    "        self.alphas = nn.Parameter(\n",
    "            torch.randn(self.num_edges, len(PRIMITIVES)) * 1e-3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = self.stem(x)\n",
    "        \n",
    "        for cell in self.cells:\n",
    "            s = cell(s, self.alphas)\n",
    "        \n",
    "        out = self.global_pool(s).view(s.size(0), -1)\n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "    \n",
    "    def genotype(self):\n",
    "        \"\"\"Derive genotype from architecture parameters\"\"\"\n",
    "        gene = []\n",
    "        \n",
    "        alphas_np = self.alphas.detach().cpu().numpy()\n",
    "        for alpha in alphas_np:\n",
    "            # Get operation with maximum weight\n",
    "            op_idx = np.argmax(alpha)\n",
    "            gene.append(PRIMITIVES[op_idx])\n",
    "        \n",
    "        return gene\n",
    "\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "class PancreaticDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_to_idx=None, img_size=224, augment=False):\n",
    "        self.root = Path(root_dir)\n",
    "        \n",
    "        # Get class folders\n",
    "        classes = sorted([d.name for d in self.root.iterdir() \n",
    "                         if d.is_dir() and not d.name.startswith('.')])\n",
    "        \n",
    "        if class_to_idx is None:\n",
    "            self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "        \n",
    "        # Collect samples\n",
    "        self.samples = []\n",
    "        for cls in classes:\n",
    "            cls_dir = self.root / cls\n",
    "            for img_path in cls_dir.glob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".png\", \".jpg\", \".jpeg\", \".bmp\"]:\n",
    "                    self.samples.append((str(img_path), self.class_to_idx[cls]))\n",
    "        \n",
    "        # Transforms\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ==================== ARCHITECT (for alpha optimization) ====================\n",
    "class Architect:\n",
    "    \"\"\"Handles architecture parameter optimization\"\"\"\n",
    "    def __init__(self, model, args):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            [model.alphas],\n",
    "            lr=args['arch_lr'],\n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=args['arch_weight_decay']\n",
    "        )\n",
    "    \n",
    "    def step(self, x_val, y_val):\n",
    "        \"\"\"Update architecture parameters\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.model(x_val)\n",
    "        loss = F.cross_entropy(logits, y_val)\n",
    "        \n",
    "        # Add entropy regularization to prevent collapse\n",
    "        alphas_softmax = F.softmax(self.model.alphas, dim=-1)\n",
    "        entropy = -(alphas_softmax * torch.log(alphas_softmax + 1e-8)).sum(dim=-1).mean()\n",
    "        \n",
    "        # We want to maximize entropy (prevent one-hot), so subtract it\n",
    "        total_loss = loss - 0.01 * entropy\n",
    "        \n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item(), entropy.item()\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "def train_search(model, train_loader, val_loader, architect, epochs=10):\n",
    "    \"\"\"Train search network with bi-level optimization\"\"\"\n",
    "    \n",
    "    # Weight optimizer\n",
    "    w_optimizer = torch.optim.SGD(\n",
    "        [p for n, p in model.named_parameters() if 'alphas' not in n],\n",
    "        lr=0.025,\n",
    "        momentum=0.9,\n",
    "        weight_decay=3e-4\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        w_optimizer, T_max=epochs, eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    val_iter = iter(val_loader)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        arch_loss_sum = 0.0\n",
    "        entropy_sum = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for step, (x_train, y_train) in enumerate(pbar):\n",
    "            # Move to device\n",
    "            x_train, y_train = x_train.to(device, non_blocking=True), y_train.to(device, non_blocking=True)\n",
    "            \n",
    "            # Get validation batch\n",
    "            try:\n",
    "                x_val, y_val = next(val_iter)\n",
    "            except StopIteration:\n",
    "                val_iter = iter(val_loader)\n",
    "                x_val, y_val = next(val_iter)\n",
    "            \n",
    "            x_val, y_val = x_val.to(device, non_blocking=True), y_val.to(device, non_blocking=True)\n",
    "            \n",
    "            # Update architecture parameters\n",
    "            arch_loss, entropy = architect.step(x_val, y_val)\n",
    "            \n",
    "            # Clear cache periodically to prevent fragmentation\n",
    "            if step % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Update network weights\n",
    "            w_optimizer.zero_grad()\n",
    "            logits = model(x_train)\n",
    "            loss = F.cross_entropy(logits, y_train)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                [p for n, p in model.named_parameters() if 'alphas' not in n], \n",
    "                5.0\n",
    "            )\n",
    "            w_optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            arch_loss_sum += arch_loss\n",
    "            entropy_sum += entropy\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'arch_loss': f'{arch_loss:.3f}',\n",
    "                'entropy': f'{entropy:.3f}'\n",
    "            })\n",
    "            \n",
    "            # Delete variables to free memory\n",
    "            del x_train, y_train, x_val, y_val, logits, loss\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Clear cache after each epoch\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_arch_loss = arch_loss_sum / len(train_loader)\n",
    "        avg_entropy = entropy_sum / len(train_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Arch Loss: {avg_arch_loss:.4f}\")\n",
    "        print(f\"  Entropy: {avg_entropy:.4f}\")\n",
    "        \n",
    "        # Save alpha checkpoint\n",
    "        np.save(f\"alphas_epoch{epoch}.npy\", model.alphas.detach().cpu().numpy())\n",
    "        \n",
    "        # Show current genotype\n",
    "        genotype = model.genotype()\n",
    "        print(f\"  Current Genotype: {genotype}\")\n",
    "        print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==================== MAIN SEARCH PROCEDURE ====================\n",
    "def main_search():\n",
    "    \"\"\"Main search procedure\"\"\"\n",
    "    \n",
    "    # Clean up checkpoint folders\n",
    "    for base in [TRAIN_DIR, TEST_DIR]:\n",
    "        chk = os.path.join(base, \".ipynb_checkpoints\")\n",
    "        if os.path.isdir(chk):\n",
    "            print(f\"Removing {chk}\")\n",
    "            shutil.rmtree(chk)\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    full_train_ds = PancreaticDataset(TRAIN_DIR, img_size=224, augment=True)\n",
    "    test_ds = PancreaticDataset(TEST_DIR, class_to_idx=full_train_ds.class_to_idx, \n",
    "                                img_size=224, augment=False)\n",
    "    \n",
    "    print(f\"Class mapping: {full_train_ds.class_to_idx}\")\n",
    "    print(f\"Total training samples: {len(full_train_ds)}\")\n",
    "    print(f\"Total test samples: {len(test_ds)}\")\n",
    "    \n",
    "    # Split train into train/val\n",
    "    train_len = int(0.8 * len(full_train_ds))\n",
    "    val_len = len(full_train_ds) - train_len\n",
    "    train_ds, val_ds = random_split(\n",
    "        full_train_ds, [train_len, val_len],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    print(f\"Search train: {len(train_ds)}, Search val: {len(val_ds)}\")\n",
    "    \n",
    "    # Create data loaders with reduced batch size for memory efficiency\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  # Reduced from 32\n",
    "                             num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=True,  # Reduced from 32\n",
    "                           num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=16, shuffle=False,  # Reduced from 32\n",
    "                            num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    # Create model with smaller configuration for memory efficiency\n",
    "    print(\"\\nCreating search network...\")\n",
    "    model = SearchNetwork(\n",
    "        C=12,           # Reduced from 16 (saves ~40% memory)\n",
    "        num_cells=4,    # Reduced from 6 (saves memory)\n",
    "        num_nodes=3,    # Reduced from 4 (fewer edges)\n",
    "        num_classes=len(full_train_ds.class_to_idx)\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create architect\n",
    "    arch_args = {\n",
    "        'arch_lr': 3e-4,\n",
    "        'arch_weight_decay': 1e-3\n",
    "    }\n",
    "    architect = Architect(model, arch_args)\n",
    "    \n",
    "    # Run search with reduced epochs for memory efficiency\n",
    "    print(\"\\nStarting architecture search...\")\n",
    "    model = train_search(model, train_loader, val_loader, architect, epochs=10)  # Reduced from 15\n",
    "    \n",
    "    # Get final genotype\n",
    "    final_genotype = model.genotype()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL GENOTYPE:\")\n",
    "    print(final_genotype)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Save results\n",
    "    np.save(\"final_alphas.npy\", model.alphas.detach().cpu().numpy())\n",
    "    with open(\"genotype.txt\", \"w\") as f:\n",
    "        f.write(\",\".join(final_genotype))\n",
    "    torch.save(model.state_dict(), \"search_model.pth\")\n",
    "    \n",
    "    print(\"\\nSaved: final_alphas.npy, genotype.txt, search_model.pth\")\n",
    "    \n",
    "    return final_genotype, full_train_ds.class_to_idx\n",
    "\n",
    "# ==================== RUN ====================\n",
    "if __name__ == \"__main__\":\n",
    "    genotype, class_to_idx = main_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
